{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-25T12:04:01.173541Z","iopub.execute_input":"2023-09-25T12:04:01.173912Z","iopub.status.idle":"2023-09-25T12:04:01.653953Z","shell.execute_reply.started":"2023-09-25T12:04:01.173883Z","shell.execute_reply":"2023-09-25T12:04:01.651234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install langdetect\n# !pip install deep_translator\n!pip install bayesian-optimization","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:04:01.656214Z","iopub.execute_input":"2023-09-25T12:04:01.656704Z","iopub.status.idle":"2023-09-25T12:04:18.522932Z","shell.execute_reply.started":"2023-09-25T12:04:01.656674Z","shell.execute_reply":"2023-09-25T12:04:18.521419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\nfrom joblib import dump\nimport logging\nimport subprocess\nfrom copy import deepcopy\nfrom tqdm import tqdm\n\n#stats\nimport scipy\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\n\n\nimport spacy\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import word_tokenize, TweetTokenizer\n# Make sure to download the needed resources if you haven't already.\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet\n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss, f1_score, roc_auc_score, precision_score, recall_score, classification_report\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\n# from langdetect import detect\n# from deep_translator import GoogleTranslator\n\n#Modelling\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom bayes_opt import BayesianOptimization","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:07:30.174567Z","iopub.execute_input":"2023-09-25T12:07:30.175017Z","iopub.status.idle":"2023-09-25T12:07:30.223530Z","shell.execute_reply.started":"2023-09-25T12:07:30.174984Z","shell.execute_reply":"2023-09-25T12:07:30.222466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialising functions that we'll use.","metadata":{}},{"cell_type":"code","source":"def read_data():\n    train = pd.read_csv('../input/jigsaw-toxic-comment-classification/train.csv').fillna(\"unknown\")\n    test = pd.read_csv('../input/jigsaw-toxic-comment-classification/test.csv').fillna(\"unknown\")\n    return train, test\n\ndef feature_engineering(data):\n#     for i,c in enumerate(train['comment_text']):\n#         try:\n#             if detect(c) != 'en':\n#                 train.iloc[i,1] = GoogleTranslator(source='auto', target='en').translate(c)\n#         except:\n#             train.iloc[i,1] = \"error\"\n#             print(f\"This row throws and error: {i}\")\n#             break\n\n    for element in data:           \n        element['total_length'] = element['comment_text'].apply(len)\n        element['capitals'] = element['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n        element['caps_vs_length'] = element.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                    axis=1)\n        element['num_exclamation_marks'] = element['comment_text'].apply(lambda comment: comment.count('!'))\n        element['num_question_marks'] = element['comment_text'].apply(lambda comment: comment.count('?'))\n        element['num_punctuation'] = element['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n        element['num_symbols'] = element['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n        element['num_words'] = element['comment_text'].apply(lambda comment: len(comment.split()))\n        element['num_unique_words'] = element['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n        element['words_vs_unique'] = element['num_unique_words'] / element['num_words']\n        element['num_smilies'] = element['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n        # ... (other features like capitals, caps_vs_length etc.)\n        # No changes here, this part looks good.\n        \n    return data\n\ndef scale_features(data, col):\n    scaler = MinMaxScaler()\n    \n    for element in data:\n        scaler_model = scaler.fit(element[col])\n        element[col] = scaler_model.fit_transform(element[col])\n        \n    return data\n\n\nstop_words_set = set(stopwords.words(\"english\"))\nlemmatizer = WordNetLemmatizer()\ntokenizer = TweetTokenizer()\n\nurl_pattern = re.compile(r'http\\S+|www\\S+|https\\S+', flags=re.MULTILINE)\npunctuation_pattern = re.compile(r'[^\\w\\s]')\nspecial_characters_pattern = re.compile(r'[^\\x00-\\x7F]+')\nextra_spaces_pattern = re.compile(r'\\s+')\n\ndef clean_tweet(tweet):\n    \"\"\"\n    Cleans the text by removing links, special characters, etc.\n    \n    Parameters:\n        tweet (str): The tweet text to clean.\n        \n    Returns:\n        str: The cleaned tweet text.\n    \"\"\"\n    # Convert to lower case\n    tweet = tweet.lower()\n    \n    # Remove URLs\n    tweet = url_pattern.sub('', tweet)\n    \n    # Remove punctuation\n    tweet = punctuation_pattern.sub('', tweet)\n\n    # Remove special characters, numbers, etc.\n    tweet = special_characters_pattern.sub('', tweet)\n    \n    # Remove extra spaces\n    tweet = extra_spaces_pattern.sub(' ', tweet).strip()\n    \n    # Tokenize\n    tweet_tokens = tokenizer.tokenize(tweet)\n    \n    # Remove stopwords and lemmatize\n    cleaned_tokens = [lemmatizer.lemmatize(word, \"v\") for word in tweet_tokens if word not in stop_words_set]\n    \n    # Join tokens into a string\n    clean_sent = \" \".join(cleaned_tokens)\n    \n    return clean_sent\n\n  \n\ndef build_tfidf(train_text):\n    tfidf_vectorizer = TfidfVectorizer(\n        ngram_range=(1, 2),\n        min_df=3,\n        max_df=0.9,\n        strip_accents='unicode',\n        use_idf=1,\n        smooth_idf=1,\n        sublinear_tf=1\n    )\n    return tfidf_vectorizer.fit(train_text)\n\ndef build_model(train_X, train_y, test_X, test_y=None):\n    # Your XGBoost model parameters\n    # ...\n    param = {}\n    param['objective'] = 'binary:logistic'\n    param['eta'] = 0.1\n    param['max_depth'] = 6\n    param['silent'] = 1\n    param['eval_metric'] = 'auc'\n    param['min_child_weight'] = 1\n    param['subsample'] = 0.7\n    param['colsample_bytree'] = 0.7\n    param['seed'] = seed_val\n    num_rounds = num_rounds\n\n    params = list(param.items())\n    \n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [(xgtrain, 'train'), (xgtest, 'test')]\n        model = xgb.train(params, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(params, xgtrain, num_rounds)\n    \n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:17:16.622420Z","iopub.execute_input":"2023-09-25T12:17:16.622809Z","iopub.status.idle":"2023-09-25T12:17:16.651310Z","shell.execute_reply.started":"2023-09-25T12:17:16.622779Z","shell.execute_reply":"2023-09-25T12:17:16.650335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Read the data into memory. Before splitting into training and validation sets, we will do some feature engineering, creating new features. We will translate the comments into english if they're not already in english. Then, the following features are engineered:\n\n ['total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks', 'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique', 'num_smilies'] \n \nFor these features, we'll also normalise them to ensure there are no wide-ranging values that'll degrade the model's performance.","metadata":{}},{"cell_type":"code","source":"train, test = read_data()\n# train_mes, valid_mes, train_l, valid_l = split_data(train)\n\n# Feature Engineering\ntrain_features, test_features = feature_engineering([train,test])\n\nfor df in [train_features, test_features]:\n    cols_with_nan = df.columns[df.isna().any()].tolist()\n        \n    for col in cols_with_nan:\n        if pd.notna(df[col]).any(): # Check if there is any non-NaN value in the column\n            df[col].fillna(df[col].mean(), inplace=True)\n        else:\n            df[col].fillna(0, inplace=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:04:37.203541Z","iopub.execute_input":"2023-09-25T12:04:37.203902Z","iopub.status.idle":"2023-09-25T12:05:07.334853Z","shell.execute_reply.started":"2023-09-25T12:04:37.203872Z","shell.execute_reply":"2023-09-25T12:05:07.333225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:05:29.756553Z","iopub.execute_input":"2023-09-25T12:05:29.757007Z","iopub.status.idle":"2023-09-25T12:05:29.789936Z","shell.execute_reply.started":"2023-09-25T12:05:29.756965Z","shell.execute_reply":"2023-09-25T12:05:29.788424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features.isna().any()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:12.414517Z","iopub.execute_input":"2023-09-25T12:10:12.415020Z","iopub.status.idle":"2023-09-25T12:10:12.473425Z","shell.execute_reply.started":"2023-09-25T12:10:12.414985Z","shell.execute_reply":"2023-09-25T12:10:12.471638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now visualise the new features that we've craeted are added to the DataFrame. Since we don't need the `id` column in prediction, we will drop them in both the train_features and test_features dataframe.","metadata":{}},{"cell_type":"code","source":"train_features.head()\ntrain_features, test_features = train_features.drop('id', axis=1), test_features.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:13.218771Z","iopub.execute_input":"2023-09-25T12:10:13.219211Z","iopub.status.idle":"2023-09-25T12:10:13.266208Z","shell.execute_reply.started":"2023-09-25T12:10:13.219158Z","shell.execute_reply":"2023-09-25T12:10:13.264832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:13.869295Z","iopub.execute_input":"2023-09-25T12:10:13.870057Z","iopub.status.idle":"2023-09-25T12:10:13.891689Z","shell.execute_reply.started":"2023-09-25T12:10:13.870017Z","shell.execute_reply":"2023-09-25T12:10:13.890168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We extract the target labels and training features to be used in training our model. As there are numeric features, we need to scale them to avoid wide-ranges. We can use sklearn's MinMaxScaler to achieve this.","metadata":{}},{"cell_type":"code","source":"labels = train_features.columns[1:7].tolist()\ncol = train_features.columns[7:].tolist()\nprint(f'training features: {col} \\ntarget labels: {labels}')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:14.317121Z","iopub.execute_input":"2023-09-25T12:10:14.317556Z","iopub.status.idle":"2023-09-25T12:10:14.326448Z","shell.execute_reply.started":"2023-09-25T12:10:14.317524Z","shell.execute_reply":"2023-09-25T12:10:14.324831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from copy import deepcopy\ndummy = deepcopy(train_features)\nscaler = MinMaxScaler()\nscaler_model = scaler.fit(train_features[col])\ndummy[col] = scaler_model.fit_transform(dummy[col])","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:21.708603Z","iopub.execute_input":"2023-09-25T12:10:21.709786Z","iopub.status.idle":"2023-09-25T12:10:21.787542Z","shell.execute_reply.started":"2023-09-25T12:10:21.709739Z","shell.execute_reply":"2023-09-25T12:10:21.786320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now view that they're normalised to a min-max range.","metadata":{}},{"cell_type":"code","source":"dummy.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:22.041109Z","iopub.execute_input":"2023-09-25T12:10:22.041540Z","iopub.status.idle":"2023-09-25T12:10:22.068125Z","shell.execute_reply.started":"2023-09-25T12:10:22.041509Z","shell.execute_reply":"2023-09-25T12:10:22.066240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use our predefined feature_scaler function to scale all the training data for train, validation and test sets. The reason behind this is to avoid training-serving skew in the feature vector.","metadata":{}},{"cell_type":"code","source":"train_features, test_features = scale_features([train_features,test_features], col)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:10:22.849569Z","iopub.execute_input":"2023-09-25T12:10:22.850014Z","iopub.status.idle":"2023-09-25T12:10:22.978728Z","shell.execute_reply.started":"2023-09-25T12:10:22.849982Z","shell.execute_reply":"2023-09-25T12:10:22.977330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's transform them into a TF-IDF vector. We will use the whole corpus to train the TF-IDF model. We will need to vectorise both the train and test set into a TF-IDF matrix to avoid training-serving skew in features. When using a TF-IDF vectorizer, it is better to fit the vectorizer on the training set only and then transform both the training and testing sets separately 123. This is because if we fit the vectorizer on the whole corpus, we might introduce data leakage and hence yield in too optimistic performance measures 2. The IDF-part of the training setâ€™s TF-IDF features will then include information from the test set already. Remember that training sets are used for learning purposes (learning is achieved through fit ()) while testing set is used in order to evaluate whether the trained model can generalize well to new unseen data points 1.\n\nTherefore, it is recommended to use fit the TF-IDF vectorizer on the training set only and then use transform on the testing set.\n","metadata":{}},{"cell_type":"code","source":"tqdm.pandas()\ntrain_features['clean_comment_text'] = train_features['comment_text'].progress_apply(clean_tweet)\ntest_features['clean_comment_text'] = test_features['comment_text'].progress_apply(clean_tweet)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:17:48.449448Z","iopub.execute_input":"2023-09-25T12:17:48.449844Z","iopub.status.idle":"2023-09-25T12:22:03.818283Z","shell.execute_reply.started":"2023-09-25T12:17:48.449816Z","shell.execute_reply":"2023-09-25T12:22:03.816921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# TF-IDF Vectorization\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=10000).fit(train_features['clean_comment_text'])\ntfidf_matrix = tfidf_vectorizer.transform(train_features['clean_comment_text'])\ntfidf_matrix_test = tfidf_vectorizer.transform(test_features['clean_comment_text'])\n\n# # Split data into training and validation sets\n# train_features, valid_features, train_labels, valid_labels = train_test_split(\n#     scipy.sparse.hstack([tfidf_matrix, train_features[col].values]),\n#     train_features[labels],\n#     test_size=0.2,\n#     random_state=42\n# )","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:22:50.738158Z","iopub.execute_input":"2023-09-25T12:22:50.738666Z","iopub.status.idle":"2023-09-25T12:24:10.042135Z","shell.execute_reply.started":"2023-09-25T12:22:50.738626Z","shell.execute_reply":"2023-09-25T12:24:10.041078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's combine the TF-IDF matrices with the existing features to form the training and test sets.","metadata":{}},{"cell_type":"code","source":"X_train = scipy.sparse.hstack([tfidf_matrix, train_features[col].values])\nX_test = scipy.sparse.hstack([tfidf_matrix_test, test_features[col].values])\n\nprint(f\"Shape of X_train: {X_train.shape} \\n\"\n     f\"Shape of X_test: {X_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:42:05.105762Z","iopub.execute_input":"2023-09-25T12:42:05.106269Z","iopub.status.idle":"2023-09-25T12:42:05.418113Z","shell.execute_reply.started":"2023-09-25T12:42:05.106229Z","shell.execute_reply":"2023-09-25T12:42:05.416636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tuning\nThis section is optional\n\nAs we can see, we have 49 dimensions of features. If we were to perform a Bayesian optimisation to find the optimal hyperparameters, it will struggle to iterate through and will take a very, very long time. Although we can technically do that for an optimal result, however given the constraint that I'm running on this Kaggle notebook and it'll timeout, I will be trading off features for training time. Thus, we need to perform a feature engineering technique called: Dimensionality reduction. We can achieve this with either:\n\n1. PCA\n2. T-SNE\n\nFor this case, we are going to use PCA for dimensionality reduction. In order to find the optimal principal components to balance between the dimensionality vs explained variance ratio tradeoff, we're going to use the `Scree Plot`. We can determine the optimal principal components through a either the `Elbow method`, or a sensible number of principal components under `n=20`","metadata":{}},{"cell_type":"code","source":"# PCA for sparse matrix\npca = TruncatedSVD(n_components=20)\nX = pca.fit_transform(X_train)\n\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\nplt.plot(range(1, 21), cumulative_variance_ratio, marker='o')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance Ratio')\nplt.title('Scree Plot')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll pick `n=20` to strike a balance between dimensionality and variance.","metadata":{}},{"cell_type":"code","source":"pca = TruncatedSVD(n_components=20)\nX_train_transformed = pca.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_transformed = pca.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_features_transformed, valid_features_transformed, train_labels, valid_labels = train_test_split(\n    X_train_transformed,\n    train_features[labels],\n    test_size=0.2,\n    random_state=42\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run this block to perform hyperparamter tuning to get the best performant parameters for our XGBoost model.","metadata":{}},{"cell_type":"code","source":"from bayes_opt import UtilityFunction\n# Bayesian Optimization function for xgboost\n\ndef xgboost_hyper_param(learning_rate, n_estimators, max_depth, subsample, colsample_bytree, gamma, label_index):\n    max_depth = int(max_depth)\n    n_estimators = int(n_estimators)\n    label = labels[label_index]\n    \n    clf = XGBClassifier(\n        objective='binary:logistic',\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        gamma=gamma,\n        eval_metric='auc',  # Change to a suitable metric for multilabel classification\n        n_jobs=-1)  # Use all available cores\n    \n    return np.mean(cross_val_score(clf, train_features_transformed, train_labels[label], cv=3, scoring='roc_auc', n_jobs=-1))\n \n# Bayesian Optimization\npbounds = {\n    'learning_rate': (0.001, 1.0),\n    'n_estimators': (100, 1000),\n    'max_depth': (5, 30),\n    'subsample': (0.7,1.0),\n    'colsample_bytree': (0.7, 1.0),\n    'gamma': (0, 5)}\n\n# Initialize UtilityFunction with appropriate parameters\nutility_function = UtilityFunction(kind='ucb', kappa=3)\n\n# Dictionary to store the optimal parameters for each label\noptimal_params_per_label = {}\n\nfor i, label in enumerate(labels):\n    print(f\"Optimizing for label: {label}\")\n    optimizer = BayesianOptimization(\n        f=lambda learning_rate, n_estimators, max_depth, subsample, colsample_bytree, gamma: \n        xgboost_hyper_param(learning_rate, n_estimators, max_depth, subsample, colsample_bytree, gamma, label_index=i), \n        pbounds=pbounds, \n        random_state=1)\n    \n    optimizer.maximize(init_points=2, n_iter=5, acquisition_function=utility_function)\n    \n    # Store the optimal parameters found for the current label\n    optimal_params = optimizer.max['params']\n    optimal_params['max_depth'] = int(optimal_params['max_depth'])\n    optimal_params['n_estimators'] = int(optimal_params['n_estimators'])\n    \n    optimal_params_per_label[label] = optimal_params\n\n# Save optimal parameters to a file or print them\nprint(\"Optimal parameters per label:\")\nfor label, params in optimal_params_per_label.items():\n    print(f\"{label}: {params}\")\n\n# Optionally, you can save the optimal parameters to a file for future reference\nimport json\nwith open('optimal_params.json', 'w') as f:\n    json.dump(optimal_params_per_label, f)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('optimal_params.json', 'w') as f:\n    json.dump(optimal_params_per_label, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we have a saved version of the optimal parameters, we'll load and parse the `.json` and use them. To save time here, I'm using a set of hard-coded hyperparameters that I've obtained from a previous session.","metadata":{}},{"cell_type":"code","source":"optimal_params_per_label = {\n\t\"toxic\": {\n\t\t\"colsample_bytree\": 0.8852800630620977,\n\t\t\"gamma\": 3.498336235897155,\n\t\t\"learning_rate\": 0.08419519215274332,\n\t\t\"max_depth\": 10,\n\t\t\"n_estimators\": 231,\n\t\t\"subsample\": 0.8043446711262454\n\t},\n\t\"severe_toxic\": {\n\t\t\"colsample_bytree\": 0.8852800630620977,\n\t\t\"gamma\": 3.498336235897155,\n\t\t\"learning_rate\": 0.08419519215274332,\n\t\t\"max_depth\": 10,\n\t\t\"n_estimators\": 231,\n\t\t\"subsample\": 0.8043446711262454\n\t},\n\t\"obscene\": {\n\t\t\"colsample_bytree\": 0.8852800630620977,\n\t\t\"gamma\": 3.498336235897155,\n\t\t\"learning_rate\": 0.08419519215274332,\n\t\t\"max_depth\": 10,\n\t\t\"n_estimators\": 231,\n\t\t\"subsample\": 0.8043446711262454\n\t},\n\t\"threat\": {\n\t\t\"colsample_bytree\": 0.9310026672723213,\n\t\t\"gamma\": 1.0984383363279045,\n\t\t\"learning_rate\": 0.01693249517530996,\n\t\t\"max_depth\": 16,\n\t\t\"n_estimators\": 478,\n\t\t\"subsample\": 0.8958329525959229\n\t},\n\t\"insult\": {\n\t\t\"colsample_bytree\": 0.8852800630620977,\n\t\t\"gamma\": 3.498336235897155,\n\t\t\"learning_rate\": 0.08419519215274332,\n\t\t\"max_depth\": 10,\n\t\t\"n_estimators\": 231,\n\t\t\"subsample\": 0.8043446711262454\n\t},\n    \"identity_hate\": {\n        \"colsample_bytree\": 0.9,\n        \"gamma\": 3.498336235897155,\n        \"learning_rate\": 0.1,\n        \"max_depth\": 10,\n        \"n_estimators\": 500,\n        \"subsample\": 0.8043446711262454\n    }\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train test split\n\nWe split the training data into training and validation sets. Since it is a multilabel problem, although we really should be using a stratified split, we are not going to do it here, as the size of the dataset is quite large and it will be practically unfeasible, given this case of a multilabel problem.","metadata":{}},{"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    X_train,\n    train_features[labels],\n    test_size=0.2,\n    random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:46:02.057380Z","iopub.execute_input":"2023-09-25T12:46:02.057852Z","iopub.status.idle":"2023-09-25T12:46:02.350315Z","shell.execute_reply.started":"2023-09-25T12:46:02.057816Z","shell.execute_reply":"2023-09-25T12:46:02.349021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Model with optimized parameters\n\nparam = {}\nparam['objective'] = 'binary:logistic'\nparam['learning_rate'] = 0.1\nparam['max_depth'] = 6\nparam['silent'] = 1\nparam['min_child_weight'] = 1\nparam['subsample'] = 0.7\nparam['colsample_bytree'] = 0.7\nparam['n_estimators'] = 500\n\n\n\nmodels = {}  # Initialize the dictionary to store your models\nfor label in labels:\n#     params = optimal_params_per_label[label]\n    model = XGBClassifier(**param)\n    \n    # Prepare the evaluation sets with only one label column at a time\n    train_eval_set = (x_train, y_train[label])\n    valid_eval_set = (x_valid, y_valid[label])\n    \n    # The eval_set parameter should be a list of (X, y) pairs\n    eval_set = [train_eval_set, valid_eval_set]\n    \n    model.fit(\n        X=x_train, \n        y=y_train[label], \n        eval_metric='auc',  # or any other suitable metric\n        eval_set=eval_set,\n        verbose=True,\n        early_stopping_rounds=25\n    )\n    \n    models[label] = model\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T12:46:44.698678Z","iopub.execute_input":"2023-09-25T12:46:44.699289Z","iopub.status.idle":"2023-09-25T13:06:16.654899Z","shell.execute_reply.started":"2023-09-25T12:46:44.699243Z","shell.execute_reply":"2023-09-25T13:06:16.653589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"from prettytable import PrettyTable\n\ndef print_metrics(models, features, labels, label_names):\n    # Initialize a PrettyTable object\n    table = PrettyTable()\n    \n    # Define column names\n    table.field_names = [\"Label\", \"Log Loss\", \"AUC\", \"Precision\", \"Recall\", \"F1 Score\"]\n    \n    # Loop over each label and calculate metrics, then add a row to the table for each label\n    for label in label_names:\n        y_pred_proba = models[label].predict_proba(features)[:, 1]\n        y_true = labels[label]\n        y_pred = models[label].predict(features)\n        \n        loss = log_loss(y_true, y_pred_proba)\n        auc = roc_auc_score(y_true, y_pred_proba)\n        precision = precision_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred)\n        report = classification_report(y_true, y_pred)\n        \n        # Add a row to the table\n        table.add_row([label, f\"{loss:.4f}\", f\"{auc:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\"])\n        print(f\"{label} \\n{report}\")\n    \n    # Print the table\n    print(table)\n\n# Call the function with appropriate arguments\nprint_metrics(models, x_valid, y_valid, labels)\n\n# for label in labels:\n#     y_pred = models[label].predict_proba(valid_features_transformed)[:, 1]\n#     loss = log_loss(valid_labels[label], y_pred)\n#     print(f\"Log Loss for {label}: {loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-25T13:07:47.254716Z","iopub.execute_input":"2023-09-25T13:07:47.255162Z","iopub.status.idle":"2023-09-25T13:07:50.779408Z","shell.execute_reply.started":"2023-09-25T13:07:47.255127Z","shell.execute_reply":"2023-09-25T13:07:50.777884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we have trouble discriminating False Negatives with a low Recall score. This is largely due to how we are approaching the problem in this case. In a multilabel problem, the binary classifier will perform poorly on `Precision` and `Recall`, as it will try to predict the majority class. In other cases, such as a multi-class classification, we can use the `scale_pos_weight` parameter of XGBClassifier to control the balance of positive and negative weights.","metadata":{}},{"cell_type":"markdown","source":"### Model Prediction","metadata":{}},{"cell_type":"code","source":"preds = np.zeros((test.shape[0], len(labels)))\nfor i, label in enumerate(labels):\n    print('fit '+ label)\n    preds[:,i] = models[label].predict(X_test)\n    \nsubm = pd.read_csv('../input/jigsaw-toxic-comment-classification/sample_submission.csv')    \nsubmid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = labels)], axis=1)\nsubmission.to_csv('xgb.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T13:08:56.418778Z","iopub.execute_input":"2023-09-25T13:08:56.419243Z","iopub.status.idle":"2023-09-25T13:09:06.928816Z","shell.execute_reply.started":"2023-09-25T13:08:56.419208Z","shell.execute_reply":"2023-09-25T13:09:06.927336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further improvements\n\nAt submission, our models scored 0.74 on the private and public leaderboards. How can we improve?\n\n1. We have observed that some of the classes have low recall. These are namely: `severe_toxic`, `threat`, and `identity_hate`. Three share a common issue: lack of data for the positive case. We need more DATA. But how? In NLP, SMOTE seem to be problematic here for some reasons: SMOTE works in feature space. It means that the output of SMOTE is not a synthetic data which is a real representative of a text inside its feature space. On one side SMOTE works with KNN and on the other hand, feature spaces for NLP problem are dramatically huge. KNN will easily fail in those huge dimensions. We can engineer an oversampling method, similar to SMOTE. The work around is:\n\n* Ignore the major class. Get a length distribution of all documents in minor class so that we generate new samples according the the true document length (number of words/phrases). We assume we want to make the size of class triple (so producing k=2 synthetic documents per original document)\n* Generate a sequence of n random integers according to that distribution. It's used for determining the length of new synthetic document.\n* For each document: Choose one integer from random length sequence and m random document whose length is close to the integer. Put tokens of all m documents in a set and randomly choose n tokens k times. these are your k new documents.\n* OR: For sentences in minority labels, do a permutation of each and every word with its TOP_N similar words\n\n2. Check for Data Drift / Training-serving Distribution skews","metadata":{}},{"cell_type":"markdown","source":"### Save each model","metadata":{}},{"cell_type":"code","source":"from joblib import dump\n\nfor label in labels:\n    dump(models[label], f'xgboost_model_{label}.joblib')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Model Training and Prediction\n# col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n# preds = np.zeros((test.shape[0], len(col)))\n\n# for i, j in enumerate(col):\n#     print(f'Fitting label {j}')\n#     model = build_model(comments_train, train_l[j], comments_valid, valid_l[j])\n#     preds[:, i] = model.predict(xgb.DMatrix(comments_test), ntree_limit=model.best_ntree_limit)\n\n#     # Optional: Save the model for future use\n#     dump(model, f'xgb_model_{j}.joblib')\n\n# # Create Submission\n# # ... (Your existing code to create submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mainblock for a script.","metadata":{}},{"cell_type":"code","source":"# def main():\n#     train, test = read_data()\n#     train_mes, valid_mes, train_l, valid_l = split_data(train)\n    \n#     # Feature Engineering\n#     train_mes, valid_mes, test = feature_engineering([train_mes, valid_mes, test])\n    \n#     # TF-IDF Transformation\n#     tfidf_vectorizer = build_tfidf(train['comment_text'])\n#     comments_train = tfidf_vectorizer.transform(train_mes)\n#     comments_valid = tfidf_vectorizer.transform(valid_mes)\n#     comments_test = tfidf_vectorizer.transform(test['comment_text'])\n    \n#     # Concatenate Features\n#     # ... (Your existing code to concatenate features)\n    \n#     # Model Training and Prediction\n#     col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n#     preds = np.zeros((test.shape[0], len(col)))\n    \n#     for i, j in enumerate(col):\n#         print(f'Fitting label {j}')\n#         model = build_model(comments_train, train_l[j], comments_valid, valid_l[j])\n#         preds[:, i] = model.predict(xgb.DMatrix(comments_test), ntree_limit=model.best_ntree_limit)\n        \n#         # Optional: Save the model for future use\n#         dump(model, f'xgb_model_{j}.joblib')\n    \n#     # Create Submission\n#     # ... (Your existing code to create submission)\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}